{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Case Study - Sentiment Analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrohortencio/applied-data-science-with-python/blob/main/Applied%20Text%20Mining%20in%20Python/Case_Study_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSSZB0mbFD3h"
      },
      "source": [
        "---\n",
        "\n",
        "_You are currently looking at **version 1.0** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-text-mining/resources/d9pwm) course resource._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV2qon5VFD4U"
      },
      "source": [
        "*Note: Some of the cells in this notebook are computationally expensive. To reduce runtime, this notebook is using a subset of the data.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKdpCb-AFD4X"
      },
      "source": [
        "# Case Study: Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBPFDa89FD4Y"
      },
      "source": [
        "### Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yfS1bQJFD4a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read in the data\n",
        "df = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
        "\n",
        "# Sample the data to speed up computation\n",
        "# Comment out this line to match with lecture\n",
        "df = df.sample(frac=0.1, random_state=10)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCGlHIt4FD4f"
      },
      "source": [
        "# Drop missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Remove any 'neutral' ratings equal to 3\n",
        "df = df[df['Rating'] != 3]\n",
        "\n",
        "# Encode 4s and 5s as 1 (rated positively)\n",
        "# Encode 1s and 2s as 0 (rated poorly)\n",
        "df['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2daZ_KsRFD4i"
      },
      "source": [
        "# Most ratings are positive\n",
        "df['Positively Rated'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "lfyeAbe2FD4k"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'], \n",
        "                                                    df['Positively Rated'], \n",
        "                                                    random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USfnlgqlFD4l"
      },
      "source": [
        "print('X_train first entry:\\n\\n', X_train.iloc[0])\n",
        "print('\\n\\nX_train shape: ', X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVJLwWsbFD4n"
      },
      "source": [
        "# CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "EfM4NgXQFD4o"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Fit the CountVectorizer to the training data\n",
        "vect = CountVectorizer().fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "JRmqariiFD4r"
      },
      "source": [
        "vect.get_feature_names()[::2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHSVEqYOFD4t"
      },
      "source": [
        "len(vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAvcI6wfFD4v"
      },
      "source": [
        "# transform the documents in the training data to a document-term matrix\n",
        "X_train_vectorized = vect.transform(X_train)\n",
        "\n",
        "X_train_vectorized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWIjIfgqFD4y"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vectorized, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2au-wIRFD4z"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Predict the transformed test documents\n",
        "predictions = model.predict(vect.transform(X_test))\n",
        "\n",
        "print('AUC: ', roc_auc_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ojXGPCkbFD40"
      },
      "source": [
        "# get the feature names as numpy array\n",
        "feature_names = np.array(vect.get_feature_names())\n",
        "\n",
        "# Sort the coefficients from the model\n",
        "sorted_coef_index = model.coef_[0].argsort()\n",
        "\n",
        "# Find the 10 smallest and 10 largest coefficients\n",
        "# The 10 largest coefficients are being indexed using [:-11:-1] \n",
        "# so the list returned is in order of largest to smallest\n",
        "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIokTwwaFD41"
      },
      "source": [
        "# Tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrxsR03YFD42"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
        "vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
        "len(vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TZ7Zm7jFD43"
      },
      "source": [
        "X_train_vectorized = vect.transform(X_train)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "predictions = model.predict(vect.transform(X_test))\n",
        "\n",
        "print('AUC: ', roc_auc_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQbF9u_jFD44"
      },
      "source": [
        "feature_names = np.array(vect.get_feature_names())\n",
        "\n",
        "sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
        "\n",
        "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
        "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGWVb4AoFD44"
      },
      "source": [
        "sorted_coef_index = model.coef_[0].argsort()\n",
        "\n",
        "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS6SyNAUFD45"
      },
      "source": [
        "# These reviews are treated the same by our current model\n",
        "print(model.predict(vect.transform(['not an issue, phone is working',\n",
        "                                    'an issue, phone is not working'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI6NX6ucFD46"
      },
      "source": [
        "# n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R10wCL8sFD47"
      },
      "source": [
        "# Fit the CountVectorizer to the training data specifiying a minimum \n",
        "# document frequency of 5 and extracting 1-grams and 2-grams\n",
        "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
        "\n",
        "X_train_vectorized = vect.transform(X_train)\n",
        "\n",
        "len(vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE1KvXfIFD48"
      },
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "predictions = model.predict(vect.transform(X_test))\n",
        "\n",
        "print('AUC: ', roc_auc_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24SJGdI2FD48"
      },
      "source": [
        "feature_names = np.array(vect.get_feature_names())\n",
        "\n",
        "sorted_coef_index = model.coef_[0].argsort()\n",
        "\n",
        "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc17cL1LFD49"
      },
      "source": [
        "# These reviews are now correctly identified\n",
        "print(model.predict(vect.transform(['not an issue, phone is working',\n",
        "                                    'an issue, phone is not working'])))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}